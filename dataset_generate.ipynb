{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating random GORGIAS code\n",
    "\n",
    "The objective of this notebook is to generate an random but syntaxically correct GORGIAS code, we won't give importance to the semantics behind each code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T01:25:55.106480Z",
     "start_time": "2025-04-02T01:25:55.100467Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "actions = [\n",
    "    # Work-related actions\n",
    "    \"attend_meeting\", \"finish_report\", \"reply_emails\", \"give_presentation\",\n",
    "    \n",
    "    # Social & leisure activities\n",
    "    \"go_to(restaurant)\", \"go_to(cinema)\", \"go_to(park)\", \"go_to(theater)\",\n",
    "    \"visit_family\", \"attend_concert\", \"travel_abroad\", \"go_shopping\",\n",
    "\n",
    "    # Health & exercise\n",
    "    \"go_gym\", \"morning_run\", \"yoga_session\", \"visit_doctor\",\n",
    "\n",
    "    # Daily tasks\n",
    "    \"buy_groceries\", \"clean_house\", \"cook_dinner\", \"read_book\",\n",
    "\n",
    "    # Transportation\n",
    "    \"take_bus\", \"ride_bike\", \"drive_car\", \"book_flight\"\n",
    "]\n",
    "\n",
    "facts = [\n",
    "    # Work-related\n",
    "    \"urgent_deadline\", \"important_meeting\", \"boss_in_office\", \"team_project_due\",\n",
    "\n",
    "    # Personal situations\n",
    "    \"feeling_sick\", \"birthday_today\", \"wedding_anniversary\", \"friend_in_town\", \"medical_appointment\",\n",
    "\n",
    "    # Weather conditions\n",
    "    \"good_weather\", \"rainy_day\", \"snowstorm\", \"hot_day\",\n",
    "\n",
    "    # Time-based events\n",
    "    \"weekend\", \"holiday_season\", \"morning_rush\", \"night_time\",\n",
    "\n",
    "    # Social dynamics\n",
    "    \"invitation_from_friend\", \"family_gathering\", \"new_restaurant_to_try\", \"concert_nearby\",\n",
    "\n",
    "    # Financial considerations\n",
    "    \"low_budget\", \"got_bonus\", \"discount_on_flight\", \"expensive_event\"\n",
    "]\n",
    "\n",
    "\n",
    "def convert_to_dynamic(term):\n",
    "    match = re.match(r\"(\\w+)(\\((.*?)\\))?\", term)\n",
    "    if match:\n",
    "        predicate = match.group(1)\n",
    "        args = match.group(3)\n",
    "        arity = len(args.split(',')) if args else 0\n",
    "        return f\"{predicate}/{arity}\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will classify any GORGIAS code as beginner level as the following : \n",
    "- simple arguments with clear rules and priorities\n",
    "- no recursion with minimal dependencies\n",
    "- a maximum of 1 or 2 layers of preferences\n",
    "- use of `complement()/2` conflict scenario, and common scenario\n",
    "- no abducible or defeasible components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T01:25:57.399217Z",
     "start_time": "2025-04-02T01:25:57.393220Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_beginner_gorgias(facts: list, actions: list) -> str:\n",
    "    functionArray = [generate_beginner_gorgias_common, generate_beginner_gorgias_conflict]\n",
    "    depth = random.randint(1, 3)\n",
    "    return functionArray[random.randint(0, 1)](depth, facts, actions)\n",
    "\n",
    "def generate_beginner_gorgias_common(depth: int, fact: list, action: list) -> str:\n",
    "    tmp = fact[:]\n",
    "    action1, action2 = random.sample(action, 2)\n",
    "    shared_condition = random.choice(tmp)\n",
    "    tmp.remove(shared_condition)\n",
    "    dynamic = f\":- dynamic {convert_to_dynamic(shared_condition)}\"\n",
    "\n",
    "    lists = list()\n",
    "    txt = f\"rule(r1, {action1}, []) :- {shared_condition}.\"\n",
    "    lists.extend([txt])\n",
    "    txt = f\"rule(r2, {action2}, []) :- {shared_condition}.\"\n",
    "    lists.extend([txt])\n",
    "    txt = f\"rule(p1, prefer(r1, r2), []).\"\n",
    "    lists.extend([txt])\n",
    "\n",
    "    if depth == 1:\n",
    "        dynamic += f\".\"\n",
    "        complement1 = f\"complement({action2}, {action1}).\"\n",
    "        complement2 = f\"complement({action1}, {action2}).\"\n",
    "        lists.extend([complement1, complement2])\n",
    "        return \"\\n\".join(lists)\n",
    "    \n",
    "    if depth == 2:\n",
    "        p2_condition = random.choice(tmp)\n",
    "        tmp.remove(p2_condition)\n",
    "        txt = f\"rule(p2, prefer(r2, r1), []) :- {p2_condition}.\"\n",
    "        lists.extend([txt])\n",
    "        txt = f\"rule(c1, prefer(p2, p1), []).\"\n",
    "        lists.extend([txt])\n",
    "        complement1 = f\"complement({action2}, {action1}).\"\n",
    "        complement2 = f\"complement({action1}, {action2}).\"\n",
    "        lists.extend([complement1, complement2])\n",
    "        dynamic += f\", {convert_to_dynamic(p2_condition)}\"\n",
    "\n",
    "    if depth == 3:\n",
    "        p2_condition = random.choice(tmp)\n",
    "        tmp.remove(p2_condition)\n",
    "        c2_condition = random.choice(tmp)\n",
    "        txt = f\"rule(p2, prefer(r2, r1), []) :- {p2_condition}.\"\n",
    "        lists.extend([txt])\n",
    "        txt = f\"rule(c1, prefer(p2, p1), []).\"\n",
    "        lists.extend([txt])\n",
    "        txt = f\"rule(c2, prefer(p1, p2), []) :- {c2_condition}.\"\n",
    "        lists.extend([txt])\n",
    "        txt = f\"rule(c3, prefer(c2, c1), []).\"\n",
    "        lists.extend([txt])\n",
    "        complement1 = f\"complement({action2}, {action1}).\"\n",
    "        complement2 = f\"complement({action1}, {action2}).\"\n",
    "        lists.extend([complement1, complement2])\n",
    "        dynamic += f\", {convert_to_dynamic(p2_condition)}, {convert_to_dynamic(c2_condition)}\"\n",
    "    lists.insert(0, dynamic)\n",
    "    return \"\\n\".join(lists)\n",
    "\n",
    "def generate_beginner_gorgias_conflict(depth: int, fact: list, action: list) -> str:\n",
    "    tmp = fact[:]\n",
    "    action1, action2 = random.sample(action, 2)\n",
    "    condition1 = random.choice(tmp)\n",
    "    tmp.remove(condition1)\n",
    "    condition2 = random.choice(tmp)\n",
    "    tmp.remove(condition2)\n",
    "    conditionConflict = random.choice(tmp)\n",
    "    tmp.remove(conditionConflict)\n",
    "    dynamic = f\":- dynamic {convert_to_dynamic(condition1)}, {convert_to_dynamic(condition2)}\"\n",
    "\n",
    "    lists = list()\n",
    "    txt = f\"rule(r1, {action1}, []) :- {condition1}.\"\n",
    "    lists.extend([txt])\n",
    "    txt = f\"rule(r2, {action2}, []) :- {condition2}.\"\n",
    "    lists.extend([txt])\n",
    "    txt = f\"rule(p1, prefer(r1, r2), []) :- {condition1}, {condition2}.\"\n",
    "    lists.extend([txt])\n",
    "    txt = f\"rule(p2, prefer(r2, r1), []) :- {condition1}, {condition2}, {conditionConflict}.\"\n",
    "    lists.extend([txt])\n",
    "    txt = f\"rule(c1, prefer(p2, p1), []).\"\n",
    "    lists.extend([txt])\n",
    "\n",
    "    if depth == 2:\n",
    "        p2_condition = random.choice(tmp)\n",
    "        tmp.remove(p2_condition)\n",
    "        txt = f\"rule(c2, prefer(p1, p2), []) :- {p2_condition}.\"\n",
    "        lists.extend([txt])\n",
    "        txt = f\"rule(c3, prefer(c2, c1), []).\"\n",
    "        lists.extend([txt])\n",
    "        complement1 = f\"complement({action2}, {action1}).\"\n",
    "        complement2 = f\"complement({action1}, {action2}).\"\n",
    "        complement3 = f\"complement({condition1}, {condition2}).\"\n",
    "        complement4 = f\"complement({condition2}, {condition1}).\"\n",
    "        lists.extend([complement1, complement2, complement3, complement4])\n",
    "        dynamic += f\", {convert_to_dynamic(p2_condition)}\"\n",
    "\n",
    "    if depth == 3:\n",
    "        p2_condition = random.choice(tmp)\n",
    "        tmp.remove(p2_condition)\n",
    "        txt = f\"rule(c2, prefer(p1, p2), []) :- {p2_condition}.\"\n",
    "        lists.extend([txt])\n",
    "        txt = f\"rule(c3, prefer(c2, c1), []).\"\n",
    "        lists.extend([txt])\n",
    "        c2_condition = random.choice(tmp)\n",
    "        # lists.extend([txt])\n",
    "        txt = f\"rule(c4, prefer(c1, c2), []) :- {c2_condition}.\"\n",
    "        lists.extend([txt])\n",
    "        txt = f\"rule(c5, prefer(c4, c3), []).\"\n",
    "        lists.extend([txt])\n",
    "        complement1 = f\"complement({action2}, {action1}).\"\n",
    "        complement2 = f\"complement({action1}, {action2}).\"\n",
    "        complement3 = f\"complement({condition1}, {condition2}).\"\n",
    "        complement4 = f\"complement({condition2}, {condition1}).\"\n",
    "        lists.extend([complement1, complement2, complement3, complement4])\n",
    "        dynamic += f\", {convert_to_dynamic(p2_condition)}, {convert_to_dynamic(c2_condition)}\"\n",
    "    dynamic += f\".\"\n",
    "    lists.insert(0, dynamic)\n",
    "    return \"\\n\".join(lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T21:35:37.572166Z",
     "start_time": "2025-03-19T21:35:37.569269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Example 1 ###\n",
      ":- dynamic family_gathering/0, medical_appointment/0\n",
      "rule(r1, buy_groceries, []) :- family_gathering.\n",
      "rule(r2, give_presentation, []) :- family_gathering.\n",
      "rule(p1, prefer(r1, r2), []).\n",
      "rule(p2, prefer(r2, r1), []) :- medical_appointment.\n",
      "rule(c1, prefer(p2, p1), []).\n",
      "complement(give_presentation, buy_groceries).\n",
      "complement(buy_groceries, give_presentation).\n",
      "\n",
      "### Example 2 ###\n",
      ":- dynamic night_time/0, friend_in_town/0.\n",
      "rule(r1, drive_car, []) :- night_time.\n",
      "rule(r2, attend_concert, []) :- friend_in_town.\n",
      "rule(p1, prefer(r1, r2), []) :- night_time, friend_in_town.\n",
      "rule(p2, prefer(r2, r1), []) :- night_time, friend_in_town, important_meeting.\n",
      "rule(c1, prefer(p2, p1), []).\n",
      "\n",
      "### Example 3 ###\n",
      "rule(r1, go_gym, []) :- expensive_event.\n",
      "rule(r2, cook_dinner, []) :- expensive_event.\n",
      "rule(p1, prefer(r1, r2), []).\n",
      "complement(cook_dinner, go_gym).\n",
      "complement(go_gym, cook_dinner).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num_examples = 3\n",
    "gorgias_examples = [generate_beginner_gorgias(facts,actions) for _ in range(num_examples)]\n",
    "\n",
    "for i, example in enumerate(gorgias_examples, 1):\n",
    "    print(f\"### Example {i} ###\\n{example}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the intermediate level we will have multi-level preference, more rules (for easy belief theories), for the preference we can add multiple conditions instead of one.\n",
    "\n",
    "A good intermediate level should be the example of Allow/deny call :\n",
    "\n",
    "```prolog\n",
    ":- dynamic phone_call/0, at_work/0, family_member/1, at_meeting/0.\n",
    "rule(r1(Call), allow(Call), []):- phone_call.\n",
    "rule(r2(Call), deny(Call), []):- phone_call.\n",
    "% Do we need to specify again the phone_call for p1 and p2 ???\n",
    "rule(p1(Call), prefer(r1(Call), r2(Call)), []):- phone_call.\n",
    "rule(p2(Call), prefer(r2(Call), r1(Call)), []):- phone_call , at_work.\n",
    "\n",
    "rule(c1(Call), prefer(p2(Call), p1(Call)), []).\n",
    "% And here too for at_work ?\n",
    "rule(c2(Call), prefer(p1(Call), p2(Call)), []):- phone_call , at_work, familly_member(Call).\n",
    "\n",
    "rule(c3(Call), prefer(c2(Call), c1(Call)), []).\n",
    "\n",
    "rule(c4(Call), prefer(c1(Call), c2(Call)), []):- phone_call , at_work, familly_member(Call), at_meeting.\n",
    "\n",
    "rule(c5(Call), prefer(c4(Call), c3(Call)), []).\n",
    "\n",
    "complement(deny(Call), allow(Call)).\n",
    "complement(allow(Call), deny(Call)).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T01:26:02.967456Z",
     "start_time": "2025-04-02T01:26:02.963043Z"
    }
   },
   "outputs": [],
   "source": [
    "# WIP\n",
    "def generate_intermediate_gorgias():\n",
    "    action1, action2 = random.sample(actions, 2)\n",
    "    \n",
    "    fact1 = random.choice(facts)\n",
    "    remaining_facts = [f for f in facts if f != fact1]\n",
    "    fact2 = random.choice(remaining_facts)\n",
    "    remaining_facts = [f for f in remaining_facts if f != fact2]\n",
    "    fact3 = random.choice(remaining_facts)\n",
    "    \n",
    "    rules = []\n",
    "    \n",
    "    rule1 = f\"rule(r1, {action1}, []) :- {fact1}.\"\n",
    "    rule2 = f\"rule(r2, {action2}, []) :- {fact1}.\"\n",
    "    \n",
    "    pref1 = f\"rule(p1, prefer(r1, r2), []) :- {fact1}, {fact3}.\"\n",
    "    pref2 = f\"rule(p2, prefer(r2, r1), []) :- {fact2}, {fact3}.\"\n",
    "    \n",
    "    conflict1 = \"rule(c1, prefer(p2, p1), []).\"\n",
    "    conflict2 = f\"rule(c2, prefer(p1, p2), []) :- {fact2}, neg({fact3}).\"\n",
    "    conflict3 = \"rule(c3, prefer(c2, c1), []).\"\n",
    "    \n",
    "    complement1 = f\"complement({action2}, {action1}).\"\n",
    "    complement2 = f\"complement({action1}, {action2}).\"\n",
    "    \n",
    "    rules.extend([rule1, rule2, pref1, pref2, conflict1, conflict2, conflict3, complement1, complement2])\n",
    "    \n",
    "    return \"\\n\".join(rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:33:58.099985Z",
     "start_time": "2025-04-01T20:33:58.095987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Example 1 ###\n",
      "rule(r1, attend_concert, []) :- morning_rush.\n",
      "rule(r2, give_presentation, []) :- morning_rush.\n",
      "rule(p1, prefer(r1, r2), []) :- morning_rush, invitation_from_friend.\n",
      "rule(p2, prefer(r2, r1), []) :- snowstorm, invitation_from_friend.\n",
      "rule(c1, prefer(p2, p1), []).\n",
      "rule(c2, prefer(p1, p2), []) :- snowstorm, neg(invitation_from_friend).\n",
      "rule(c3, prefer(c2, c1), []).\n",
      "complement(give_presentation, attend_concert).\n",
      "complement(attend_concert, give_presentation).\n",
      "\n",
      "### Example 2 ###\n",
      "rule(r1, go_to(theater), []) :- feeling_sick.\n",
      "rule(r2, drive_car, []) :- feeling_sick.\n",
      "rule(p1, prefer(r1, r2), []) :- feeling_sick, invitation_from_friend.\n",
      "rule(p2, prefer(r2, r1), []) :- medical_appointment, invitation_from_friend.\n",
      "rule(c1, prefer(p2, p1), []).\n",
      "rule(c2, prefer(p1, p2), []) :- medical_appointment, neg(invitation_from_friend).\n",
      "rule(c3, prefer(c2, c1), []).\n",
      "complement(drive_car, go_to(theater)).\n",
      "complement(go_to(theater), drive_car).\n",
      "\n",
      "### Example 3 ###\n",
      "rule(r1, go_to(restaurant), []) :- expensive_event.\n",
      "rule(r2, go_gym, []) :- expensive_event.\n",
      "rule(p1, prefer(r1, r2), []) :- expensive_event, birthday_today.\n",
      "rule(p2, prefer(r2, r1), []) :- urgent_deadline, birthday_today.\n",
      "rule(c1, prefer(p2, p1), []).\n",
      "rule(c2, prefer(p1, p2), []) :- urgent_deadline, neg(birthday_today).\n",
      "rule(c3, prefer(c2, c1), []).\n",
      "complement(go_gym, go_to(restaurant)).\n",
      "complement(go_to(restaurant), go_gym).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num_examples = 3\n",
    "gorgias_intermediate_examples = [generate_intermediate_gorgias() for _ in range(num_examples)]\n",
    "\n",
    "for i, example in enumerate(gorgias_intermediate_examples, 1):\n",
    "    print(f\"### Example {i} ###\\n{example}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code automates the creation of 100 Gorgias code examples and saves them in a structured CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:39:08.848331Z",
     "start_time": "2025-04-01T20:39:08.843095Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "num_examples = 100\n",
    "\n",
    "gorgias_examples = [generate_beginner_gorgias() for _ in range(num_examples)]\n",
    "\n",
    "with open(\"gorgias_beginner_examples.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Example Number\", \"Gorgias Code\"])\n",
    "    for i, example in enumerate(gorgias_examples, start=1):\n",
    "        writer.writerow([i, example])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code reads a CSV file containing Gorgias code examples, translates each example into clear English using the OpenAI Chat API, and then writes the original code along with its translation into a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T21:45:49.633034Z",
     "start_time": "2025-03-19T21:42:38.474982Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      5\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msk-proj-KMU8hSnWjESQ6_9hWVG29IXmG7qCMFuJNEwzNJqdAh6qMPcgXwHsBuC-s7Q7wQrw5e3tx00v0eT3BlbkFJc3ZLPSXhAW3CI4VIVAoCoo3QtUF7lx4A-Rn85SAn7nVL7uOsEaW_tZjNM3CG8r9zSBNfTrOVAA\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_types\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NOT_GIVEN, Omit, NoneType, NotGiven, Transport, ProxiesTypes\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m file_from_path\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Client, OpenAI, Stream, Timeout, Transport, AsyncClient, AsyncOpenAI, AsyncStream, RequestOptions\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __title__, __version__\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\_client.py:28\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     is_given,\n\u001b[0;32m     24\u001b[0m     is_mapping,\n\u001b[0;32m     25\u001b[0m     get_async_library,\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresources\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m files, images, models, batches, embeddings, completions, moderations\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_streaming\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Stream \u001b[38;5;28;01mas\u001b[39;00m Stream, AsyncStream \u001b[38;5;28;01mas\u001b[39;00m AsyncStream\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIError, APIStatusError\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\resources\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbeta\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     Beta,\n\u001b[0;32m      5\u001b[0m     AsyncBeta,\n\u001b[0;32m      6\u001b[0m     BetaWithRawResponse,\n\u001b[0;32m      7\u001b[0m     AsyncBetaWithRawResponse,\n\u001b[0;32m      8\u001b[0m     BetaWithStreamingResponse,\n\u001b[0;32m      9\u001b[0m     AsyncBetaWithStreamingResponse,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     12\u001b[0m     Chat,\n\u001b[0;32m     13\u001b[0m     AsyncChat,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     AsyncChatWithStreamingResponse,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     Audio,\n\u001b[0;32m     21\u001b[0m     AsyncAudio,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     AsyncAudioWithStreamingResponse,\n\u001b[0;32m     26\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\resources\\beta\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbeta\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     Beta,\n\u001b[0;32m      5\u001b[0m     AsyncBeta,\n\u001b[0;32m      6\u001b[0m     BetaWithRawResponse,\n\u001b[0;32m      7\u001b[0m     AsyncBetaWithRawResponse,\n\u001b[0;32m      8\u001b[0m     BetaWithStreamingResponse,\n\u001b[0;32m      9\u001b[0m     AsyncBetaWithStreamingResponse,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mthreads\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     12\u001b[0m     Threads,\n\u001b[0;32m     13\u001b[0m     AsyncThreads,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     AsyncThreadsWithStreamingResponse,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massistants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     Assistants,\n\u001b[0;32m     21\u001b[0m     AsyncAssistants,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     AsyncAssistantsWithStreamingResponse,\n\u001b[0;32m     26\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\resources\\beta\\beta.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cached_property\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chat, AsyncChat\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massistants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     Assistants,\n\u001b[0;32m      9\u001b[0m     AsyncAssistants,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     AsyncAssistantsWithStreamingResponse,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_resource\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SyncAPIResource, AsyncAPIResource\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\resources\\beta\\chat\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chat, AsyncChat\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompletions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Completions, AsyncCompletions\n\u001b[0;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompletions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsyncCompletions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsyncChat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\resources\\beta\\chat\\chat.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cached_property\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompletions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Completions, AsyncCompletions\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_resource\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SyncAPIResource, AsyncAPIResource\n\u001b[0;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsyncChat\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\resources\\beta\\chat\\completions.py:27\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_parsing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     ResponseFormatT,\n\u001b[0;32m     22\u001b[0m     validate_input_tools \u001b[38;5;28;01mas\u001b[39;00m _validate_input_tools,\n\u001b[0;32m     23\u001b[0m     parse_chat_completion \u001b[38;5;28;01mas\u001b[39;00m _parse_chat_completion,\n\u001b[0;32m     24\u001b[0m     type_to_response_format_param \u001b[38;5;28;01mas\u001b[39;00m _type_to_response_format,\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatModel\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatCompletionStreamManager, AsyncChatCompletionStreamManager\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshared_params\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Metadata, ReasoningEffort\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_completion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatCompletion\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\lib\\streaming\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_assistants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     AssistantEventHandler \u001b[38;5;28;01mas\u001b[39;00m AssistantEventHandler,\n\u001b[0;32m      3\u001b[0m     AssistantEventHandlerT \u001b[38;5;28;01mas\u001b[39;00m AssistantEventHandlerT,\n\u001b[0;32m      4\u001b[0m     AssistantStreamManager \u001b[38;5;28;01mas\u001b[39;00m AssistantStreamManager,\n\u001b[0;32m      5\u001b[0m     AsyncAssistantEventHandler \u001b[38;5;28;01mas\u001b[39;00m AsyncAssistantEventHandler,\n\u001b[0;32m      6\u001b[0m     AsyncAssistantEventHandlerT \u001b[38;5;28;01mas\u001b[39;00m AsyncAssistantEventHandlerT,\n\u001b[0;32m      7\u001b[0m     AsyncAssistantStreamManager \u001b[38;5;28;01mas\u001b[39;00m AsyncAssistantStreamManager,\n\u001b[0;32m      8\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\lib\\streaming\\_assistants.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m construct_type\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_streaming\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Stream, AsyncStream\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbeta\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AssistantStreamEvent\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbeta\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mthreads\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     Run,\n\u001b[0;32m     17\u001b[0m     Text,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     MessageContentDelta,\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbeta\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mthreads\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mruns\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RunStep, ToolCall, RunStepDelta, ToolCallDelta\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\types\\beta\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mthread\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Thread \u001b[38;5;28;01mas\u001b[39;00m Thread\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massistant\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Assistant \u001b[38;5;28;01mas\u001b[39;00m Assistant\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction_tool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FunctionTool \u001b[38;5;28;01mas\u001b[39;00m FunctionTool\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massistant_tool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AssistantTool \u001b[38;5;28;01mas\u001b[39;00m AssistantTool\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\types\\beta\\assistant.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massistant_tool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AssistantTool\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshared\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Metadata\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massistant_response_format_option\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AssistantResponseFormatOption\n\u001b[0;32m     11\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToolResources\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToolResourcesCodeInterpreter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToolResourcesFileSearch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mToolResourcesCodeInterpreter\u001b[39;00m(BaseModel):\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:969\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1091\u001b[0m, in \u001b[0;36mpath_stats\u001b[1;34m(self, path)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import openai\n",
    "import time\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def translate_gorgias_to_nl(gorgias_code):\n",
    "\n",
    "    prompt = f\"Please translate the following Gorgias program into clear English, without including any additional built-in rules or extra explanations:\\n\\n{gorgias_code}\\n\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages,\n",
    "            temperature=0.5,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        translation = response.choices[0].message['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API call: {e}\")\n",
    "        translation = \"Error in translation.\"\n",
    "\n",
    "    return translation\n",
    "\n",
    "input_file = \"gorgias_beginner_examples.csv\"\n",
    "\n",
    "output_file = \"gorgias_beginner_nl_pairs.csv\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "     open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as outfile:\n",
    "\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = [\"Example Number\", \"Gorgias Code\", \"NL Translation\"]\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        example_number = row[\"Example Number\"]\n",
    "        gorgias_code = row[\"Gorgias Code\"]\n",
    "\n",
    "        print(f\"Processing example {example_number}...\")\n",
    "        nl_translation = translate_gorgias_to_nl(gorgias_code)\n",
    "\n",
    "        writer.writerow({\n",
    "            \"Example Number\": example_number,\n",
    "            \"Gorgias Code\": gorgias_code,\n",
    "            \"NL Translation\": nl_translation\n",
    "        })\n",
    "\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code does the same thing as the previous one, but with an example in the prompt.  This results in significantly improved, more human-like, and syntactically correct responses from the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T01:43:38.039134Z",
     "start_time": "2025-04-02T01:42:59.636676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example 1...\n",
      "Processing example 2...\n",
      "Processing example 3...\n",
      "Processing example 4...\n",
      "Processing example 5...\n",
      "Processing example 6...\n",
      "Processing example 7...\n",
      "Processing example 8...\n",
      "Processing example 9...\n",
      "Processing example 10...\n",
      "Processing example 11...\n",
      "Processing example 12...\n",
      "Processing example 13...\n",
      "Processing example 14...\n",
      "Processing example 15...\n",
      "Processing example 16...\n",
      "Processing example 17...\n",
      "Processing example 18...\n",
      "Processing example 19...\n",
      "Processing example 20...\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import openai\n",
    "import time\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def translate_gorgias_to_nl(gorgias_code):\n",
    "\n",
    "    prompt = f\"\"\"The Gorgias program :\n",
    "\n",
    ":- dynamic go_out/0, stay_home/0, nice_weather/0, nice_movie_tv/0, invitation_from_friend/0.\n",
    "rule(r1, go_out, []) :- nice_weather.\n",
    "rule(r2, stay_home, []) :- nice_weather.\n",
    "rule(p1, prefer(r1,r2), []).\n",
    "rule(p2, prefer(r2,r1), []) :- nice_movie_tv.\n",
    "rule(c1, prefer(p2,p1), []).\n",
    "rule(c2, prefer(p1,p2) :- invitation_from_friend, []).\n",
    "rule(c3, prefer(c2,c1), []).\n",
    "complement(go_out, stay_home).\n",
    "complement(stay_home, go_out).\n",
    "\n",
    "translates to English as \"When it is nice weather I can go out or stay home. Generally, I prefer to go out but if there is a nice movie on TV I prefer to stay home. However, if I have an invitation from a friend I prefer to go out. I can't at the same time go out and stay home.\".\n",
    "\n",
    "The Gorgias program :\n",
    "\n",
    ":- dynamic accept_call/0, deny_call/0, from_family_member/0.\n",
    "rule(r1, accept_call, []) :-\n",
    "rule(r2, deny_call, []) :-\n",
    "rule(p1, prefer(r2, r1), []).\n",
    "rule(p2, prefer(r1, r2), []) :- from_family_member.\n",
    "rule(c1, prefer(p2, p1), []).\n",
    "complement(accept_call, deny_call).\n",
    "complement(deny_call, accept_call).\n",
    "\n",
    "translates to English as \"I can either accept or deny the call. Generally, I prefer to deny the call but if it is from a family member I prefer to accept the call. I can't at the same time accept and deny the call.\".\n",
    "\n",
    "Please translate the following Gorgias program into clear English by taking into account only the syntax, by disregarding semantic. Output only the translated text, without any labels, introductions, or explanations. :\\n\\n{gorgias_code}\\n\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages,\n",
    "            temperature=0.5,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        translation = response.choices[0].message['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API call: {e}\")\n",
    "        translation = \"Error in translation.\"\n",
    "\n",
    "    return translation\n",
    "\n",
    "input_file = \"gorgias_beginner_examples_modified.csv\"\n",
    "\n",
    "output_file = \"gorgias_beginner_nl_pairs_modified_prompt29.csv\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "     open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as outfile:\n",
    "\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = [\"Example Number\", \"Gorgias Code\", \"NL Translation\"]\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        example_number = row[\"Example Number\"]\n",
    "        gorgias_code = row[\"Gorgias Code\"]\n",
    "\n",
    "        print(f\"Processing example {example_number}...\")\n",
    "        nl_translation = translate_gorgias_to_nl(gorgias_code)\n",
    "\n",
    "        writer.writerow({\n",
    "            \"Example Number\": example_number,\n",
    "            \"Gorgias Code\": gorgias_code,\n",
    "            \"NL Translation\": nl_translation\n",
    "        })\n",
    "\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T01:29:19.284051Z",
     "start_time": "2025-04-02T01:28:32.056965Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mmuzz\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 29.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.28 seconds, 71.43 sentences/sec\n",
      "Exemple 1:\n",
      "  Precision : 0.9473\n",
      "  Recall    : 0.9547\n",
      "  F1        : 0.9510\n",
      "----------\n",
      "Exemple 2:\n",
      "  Precision : 0.9717\n",
      "  Recall    : 0.9687\n",
      "  F1        : 0.9702\n",
      "----------\n",
      "Exemple 3:\n",
      "  Precision : 0.9351\n",
      "  Recall    : 0.9328\n",
      "  F1        : 0.9339\n",
      "----------\n",
      "Exemple 4:\n",
      "  Precision : 0.9830\n",
      "  Recall    : 0.9839\n",
      "  F1        : 0.9834\n",
      "----------\n",
      "Exemple 5:\n",
      "  Precision : 0.9517\n",
      "  Recall    : 0.9486\n",
      "  F1        : 0.9502\n",
      "----------\n",
      "Exemple 6:\n",
      "  Precision : 0.9760\n",
      "  Recall    : 0.9750\n",
      "  F1        : 0.9755\n",
      "----------\n",
      "Exemple 7:\n",
      "  Precision : 0.9591\n",
      "  Recall    : 0.9634\n",
      "  F1        : 0.9613\n",
      "----------\n",
      "Exemple 8:\n",
      "  Precision : 0.9790\n",
      "  Recall    : 0.9803\n",
      "  F1        : 0.9796\n",
      "----------\n",
      "Exemple 9:\n",
      "  Precision : 0.9416\n",
      "  Recall    : 0.9577\n",
      "  F1        : 0.9496\n",
      "----------\n",
      "Exemple 10:\n",
      "  Precision : 0.9867\n",
      "  Recall    : 0.9851\n",
      "  F1        : 0.9859\n",
      "----------\n",
      "Exemple 11:\n",
      "  Precision : 0.9528\n",
      "  Recall    : 0.9293\n",
      "  F1        : 0.9409\n",
      "----------\n",
      "Exemple 12:\n",
      "  Precision : 0.9809\n",
      "  Recall    : 0.9799\n",
      "  F1        : 0.9804\n",
      "----------\n",
      "Exemple 13:\n",
      "  Precision : 0.9820\n",
      "  Recall    : 0.9835\n",
      "  F1        : 0.9828\n",
      "----------\n",
      "Exemple 14:\n",
      "  Precision : 0.9464\n",
      "  Recall    : 0.9233\n",
      "  F1        : 0.9347\n",
      "----------\n",
      "Exemple 15:\n",
      "  Precision : 1.0000\n",
      "  Recall    : 1.0000\n",
      "  F1        : 1.0000\n",
      "----------\n",
      "Exemple 16:\n",
      "  Precision : 0.9667\n",
      "  Recall    : 0.9704\n",
      "  F1        : 0.9685\n",
      "----------\n",
      "Exemple 17:\n",
      "  Precision : 0.9503\n",
      "  Recall    : 0.9410\n",
      "  F1        : 0.9456\n",
      "----------\n",
      "Exemple 18:\n",
      "  Precision : 0.9100\n",
      "  Recall    : 0.9157\n",
      "  F1        : 0.9128\n",
      "----------\n",
      "Exemple 19:\n",
      "  Precision : 0.9646\n",
      "  Recall    : 0.9608\n",
      "  F1        : 0.9627\n",
      "----------\n",
      "Exemple 20:\n",
      "  Precision : 0.9641\n",
      "  Recall    : 0.9623\n",
      "  F1        : 0.9632\n",
      "----------\n",
      "F1 moyen sur tous les exemples : 0.9616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bert_score import score\n",
    "\n",
    "\n",
    "csv_file = \"gorgias_beginner_nl_pairs_modified_prompt22.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "references = df[\"Manual NL Translation\"].tolist()\n",
    "candidates = df[\"NL Translation\"].tolist()\n",
    "\n",
    "P, R, F1 = score(candidates, references, lang=\"en\", verbose=True)\n",
    "\n",
    "for i, (p, r, f1) in enumerate(zip(P, R, F1)):\n",
    "    print(f\"Exemple {df['Example Number'][i]}:\")\n",
    "    print(f\"  Precision : {p.item():.4f}\")\n",
    "    print(f\"  Recall    : {r.item():.4f}\")\n",
    "    print(f\"  F1        : {f1.item():.4f}\")\n",
    "    print(\"----------\")\n",
    "\n",
    "avg_f1 = F1.mean().item()\n",
    "print(f\"F1 moyen sur tous les exemples : {avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T01:29:44.312861Z",
     "start_time": "2025-04-02T01:29:44.293871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes du fichier CSV : ['Example Number', 'Gorgias Code', 'NL Translation', 'Manual NL Translation']\n",
      "BLEU score par exemple :\n",
      "Exemple 1: 0.6128\n",
      "Exemple 2: 0.6488\n",
      "Exemple 3: 0.2893\n",
      "Exemple 4: 0.7869\n",
      "Exemple 5: 0.6722\n",
      "Exemple 6: 0.6922\n",
      "Exemple 7: 0.5812\n",
      "Exemple 8: 0.8339\n",
      "Exemple 9: 0.4254\n",
      "Exemple 10: 0.8733\n",
      "Exemple 11: 0.4874\n",
      "Exemple 12: 0.7440\n",
      "Exemple 13: 0.7668\n",
      "Exemple 14: 0.2813\n",
      "Exemple 15: 1.0000\n",
      "Exemple 16: 0.7596\n",
      "Exemple 17: 0.3723\n",
      "Exemple 18: 0.2896\n",
      "Exemple 19: 0.6729\n",
      "Exemple 20: 0.6446\n",
      "\n",
      "Corpus BLEU score: 0.6736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mmuzz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "csv_file = \"gorgias_beginner_nl_pairs_modified_prompt22.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "print(\"Colonnes du fichier CSV :\", df.columns.tolist())\n",
    "\n",
    "references = df[\"Manual NL Translation\"].tolist()       # Texte de référence\n",
    "candidates = df[\"NL Translation\"].tolist()       # Traduction candidate\n",
    "\n",
    "tokenized_references = [nltk.word_tokenize(ref) for ref in references]\n",
    "tokenized_candidates = [nltk.word_tokenize(cand) for cand in candidates]\n",
    "\n",
    "list_of_references = [[ref_tokens] for ref_tokens in tokenized_references]\n",
    "\n",
    "print(\"BLEU score par exemple :\")\n",
    "for i, (refs, cand) in enumerate(zip(list_of_references, tokenized_candidates)):\n",
    "    bleu = sentence_bleu(refs, cand)\n",
    "    print(f\"Exemple {df['Example Number'][i]}: {bleu:.4f}\")\n",
    "\n",
    "corpus_bleu_score = corpus_bleu(list_of_references, tokenized_candidates)\n",
    "print(f\"\\nCorpus BLEU score: {corpus_bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T01:36:59.519610Z",
     "start_time": "2025-04-02T01:36:46.010810Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wmt20-comet-da is already in cache.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\mmuzz\\.cache\\torch\\unbabel_comet\\wmt20-comet-da\\checkpoints\\model.ckpt`\n",
      "Encoder model frozen.\n",
      "C:\\Users\\mmuzz\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\core\\saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\mmuzz\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:03<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple 1: COMET score = 0.6903\n",
      "Exemple 2: COMET score = 0.7221\n",
      "Exemple 3: COMET score = 0.5315\n",
      "Exemple 4: COMET score = 0.7361\n",
      "Exemple 5: COMET score = 0.5841\n",
      "Exemple 6: COMET score = 0.6882\n",
      "Exemple 7: COMET score = 0.5743\n",
      "Exemple 8: COMET score = 0.7539\n",
      "Exemple 9: COMET score = 0.6205\n",
      "Exemple 10: COMET score = 0.8056\n",
      "Exemple 11: COMET score = 0.4532\n",
      "Exemple 12: COMET score = 0.8351\n",
      "Exemple 13: COMET score = 0.8690\n",
      "Exemple 14: COMET score = 0.0860\n",
      "Exemple 15: COMET score = 0.9535\n",
      "Exemple 16: COMET score = 0.6899\n",
      "Exemple 17: COMET score = 0.2492\n",
      "Exemple 18: COMET score = 0.2653\n",
      "Exemple 19: COMET score = 0.7713\n",
      "Exemple 20: COMET score = 0.7763\n",
      "\n",
      "COMET score moyen sur l'ensemble des exemples : 0.6328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "model_path = download_model(\"wmt20-comet-da\")\n",
    "model = load_from_checkpoint(model_path)\n",
    "\n",
    "csv_file = \"gorgias_beginner_nl_pairs_modified_prompt22.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "references = df[\"Manual NL Translation\"].tolist()\n",
    "candidates = df[\"NL Translation\"].tolist()\n",
    "\n",
    "data = []\n",
    "for ref, cand in zip(references, candidates):\n",
    "    data.append({\n",
    "        \"src\": \"\",\n",
    "        \"mt\": cand,\n",
    "        \"ref\": ref\n",
    "    })\n",
    "\n",
    "output = model.predict(data, batch_size=8, gpus=0)\n",
    "scores = output[\"scores\"] if isinstance(output, dict) and \"scores\" in output else output\n",
    "\n",
    "for i, score in enumerate(scores):\n",
    "    print(f\"Exemple {df['Example Number'][i]}: COMET score = {float(score):.4f}\")\n",
    "\n",
    "avg_score = sum(float(s) for s in scores) / len(scores)\n",
    "print(f\"\\nCOMET score moyen sur l'ensemble des exemples : {avg_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
