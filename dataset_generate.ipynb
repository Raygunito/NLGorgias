{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating random GORGIAS code\n",
    "\n",
    "The objective of this notebook is to generate an random but syntaxically correct GORGIAS code, we won't give importance to the semantics behind each code."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T01:25:55.106480Z",
     "start_time": "2025-04-02T01:25:55.100467Z"
    }
   },
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "actions = [\n",
    "    # Work-related actions\n",
    "    \"attend_meeting\", \"finish_report\", \"reply_emails\", \"give_presentation\",\n",
    "    \n",
    "    # Social & leisure activities\n",
    "    \"go_to(restaurant)\", \"go_to(cinema)\", \"go_to(park)\", \"go_to(theater)\",\n",
    "    \"visit_family\", \"attend_concert\", \"travel_abroad\", \"go_shopping\",\n",
    "\n",
    "    # Health & exercise\n",
    "    \"go_gym\", \"morning_run\", \"yoga_session\", \"visit_doctor\",\n",
    "\n",
    "    # Daily tasks\n",
    "    \"buy_groceries\", \"clean_house\", \"cook_dinner\", \"read_book\",\n",
    "\n",
    "    # Transportation\n",
    "    \"take_bus\", \"ride_bike\", \"drive_car\", \"book_flight\"\n",
    "]\n",
    "\n",
    "facts = [\n",
    "    # Work-related\n",
    "    \"urgent_deadline\", \"important_meeting\", \"boss_in_office\", \"team_project_due\",\n",
    "\n",
    "    # Personal situations\n",
    "    \"feeling_sick\", \"birthday_today\", \"wedding_anniversary\", \"friend_in_town\", \"medical_appointment\",\n",
    "\n",
    "    # Weather conditions\n",
    "    \"good_weather\", \"rainy_day\", \"snowstorm\", \"hot_day\",\n",
    "\n",
    "    # Time-based events\n",
    "    \"weekend\", \"holiday_season\", \"morning_rush\", \"night_time\",\n",
    "\n",
    "    # Social dynamics\n",
    "    \"invitation_from_friend\", \"family_gathering\", \"new_restaurant_to_try\", \"concert_nearby\",\n",
    "\n",
    "    # Financial considerations\n",
    "    \"low_budget\", \"got_bonus\", \"discount_on_flight\", \"expensive_event\"\n",
    "]\n",
    "\n",
    "\n",
    "def convert_to_dynamic(term):\n",
    "    match = re.match(r\"(\\w+)(\\((.*?)\\))?\", term)\n",
    "    if match:\n",
    "        predicate = match.group(1)\n",
    "        args = match.group(3)\n",
    "        arity = len(args.split(',')) if args else 0\n",
    "        return f\"{predicate}/{arity}\"\n",
    "    return None"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will classify any GORGIAS code as beginner level as the following : \n",
    "- simple arguments with clear rules and priorities\n",
    "- no recursion with minimal dependencies\n",
    "- a maximum of 1 or 2 layers of preferences\n",
    "- use of `complement()/2` conflict scenario, and common scenario\n",
    "- no abducible or defeasible components"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T01:25:57.399217Z",
     "start_time": "2025-04-02T01:25:57.393220Z"
    }
   },
   "source": [
    "def generate_beginner_gorgias():\n",
    "    action1, action2 = random.sample(actions, 2)\n",
    "    shared_condition = random.choice(facts)\n",
    "\n",
    "    remaining_facts = [fact for fact in facts if fact != shared_condition]\n",
    "    p2_condition = random.choice(remaining_facts)\n",
    "\n",
    "    remaining_facts = [fact for fact in remaining_facts if fact != p2_condition]\n",
    "    c2_condition = random.choice(remaining_facts)\n",
    "\n",
    "    # build dynamic\n",
    "    if convert_to_dynamic(action1) == convert_to_dynamic(action2):\n",
    "        # in case we have the same action for example : go_to(restaurant) and go_to(cinema) we should have only one dynamic\n",
    "        dynamic = f\":- dynamic {convert_to_dynamic(action1)}\"\n",
    "    else:\n",
    "        dynamic = f\":- dynamic {convert_to_dynamic(action1)}, {convert_to_dynamic(action2)}\"\n",
    "    \n",
    "    isConflictScenario = random.choice([True, False])\n",
    "\n",
    "    if isConflictScenario:  # Conflict scenario : 2 actions that are mutually exclusive\n",
    "        rule1 = f\"rule(r1, {action1}, []) :-.\"\n",
    "        rule2 = f\"rule(r2, {action2}, []) :-.\"\n",
    "    else:  # common scenario : 2 actions that have a common condition\n",
    "        rule1 = f\"rule(r1, {action1}, []) :- {shared_condition}.\"\n",
    "        rule2 = f\"rule(r2, {action2}, []) :- {shared_condition}.\"\n",
    "        dynamic += f\", {convert_to_dynamic(shared_condition)}\"\n",
    "\n",
    "    pref1 = f\"rule(p1, prefer(r1, r2), []).\"\n",
    "\n",
    "    # 1 = just p1, 2 = add p2 & c1, 3 = full depth\n",
    "    depth = random.choice([1, 2, 3])\n",
    "\n",
    "    rules = [rule1, rule2, pref1]\n",
    "\n",
    "    if depth >= 2:\n",
    "        pref2 = f\"rule(p2, prefer(r2, r1), []) :- {p2_condition}.\"\n",
    "        conflict1 = \"rule(c1, prefer(p2, p1), []).\"\n",
    "        rules.extend([pref2, conflict1])\n",
    "        dynamic += f\", {convert_to_dynamic(p2_condition)}\"\n",
    "\n",
    "    if depth == 3:\n",
    "        conflict2 = f\"rule(c2, prefer(p1, p2), []) :- {c2_condition}.\"\n",
    "        conflict3 = \"rule(c3, prefer(c2, c1), []).\"\n",
    "        rules.extend([conflict2, conflict3])\n",
    "        dynamic += f\", {convert_to_dynamic(c2_condition)}\"\n",
    "\n",
    "    dynamic += f\".\"\n",
    "    complement1 = f\"complement({action2}, {action1}).\"\n",
    "    complement2 = f\"complement({action1}, {action2}).\"\n",
    "    rules.extend([complement1, complement2])\n",
    "\n",
    "    rules.insert(0, dynamic)\n",
    "\n",
    "    return \"\\n\".join(rules)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T21:35:37.572166Z",
     "start_time": "2025-03-19T21:35:37.569269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Example 1 ###\n",
      ":- dynamic reply_emails/0, go_to/1, good_weather/0, rainy_day/0, medical_appointment/0.\n",
      "rule(r1, reply_emails, []) :- good_weather.\n",
      "rule(r2, go_to(theater), []) :- good_weather.\n",
      "rule(p1, prefer(r1, r2), []).\n",
      "rule(p2, prefer(r2, r1), []) :- rainy_day.\n",
      "rule(c1, prefer(p2, p1), []).\n",
      "rule(c2, prefer(p1, p2), []) :- medical_appointment.\n",
      "rule(c3, prefer(c2, c1), []).\n",
      "complement(go_to(theater), reply_emails).\n",
      "complement(reply_emails, go_to(theater)).\n",
      "\n",
      "### Example 2 ###\n",
      ":- dynamic go_to/1, attend_concert/0, invitation_from_friend/0, urgent_deadline/0.\n",
      "rule(r1, go_to(restaurant), []) :-.\n",
      "rule(r2, attend_concert, []) :-.\n",
      "rule(p1, prefer(r1, r2), []).\n",
      "rule(p2, prefer(r2, r1), []) :- invitation_from_friend.\n",
      "rule(c1, prefer(p2, p1), []).\n",
      "rule(c2, prefer(p1, p2), []) :- urgent_deadline.\n",
      "rule(c3, prefer(c2, c1), []).\n",
      "complement(attend_concert, go_to(restaurant)).\n",
      "complement(go_to(restaurant), attend_concert).\n",
      "\n",
      "### Example 3 ###\n",
      ":- dynamic morning_run/0, go_to/1.\n",
      "rule(r1, morning_run, []) :-.\n",
      "rule(r2, go_to(theater), []) :-.\n",
      "rule(p1, prefer(r1, r2), []).\n",
      "complement(go_to(theater), morning_run).\n",
      "complement(morning_run, go_to(theater)).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num_examples = 3\n",
    "gorgias_examples = [generate_beginner_gorgias() for _ in range(num_examples)]\n",
    "\n",
    "for i, example in enumerate(gorgias_examples, 1):\n",
    "    print(f\"### Example {i} ###\\n{example}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the intermediate level we will have multi-level preference, more rules (for easy belief theories), for the preference we can add multiple conditions instead of one.\n",
    "\n",
    "A good intermediate level should be the example of Allow/deny call :\n",
    "\n",
    "```prolog\n",
    ":- dynamic phone_call/0, at_work/0, family_member/1, at_meeting/0.\n",
    "rule(r1(Call), allow(Call), []):- phone_call.\n",
    "rule(r2(Call), deny(Call), []):- phone_call.\n",
    "% Do we need to specify again the phone_call for p1 and p2 ???\n",
    "rule(p1(Call), prefer(r1(Call), r2(Call)), []):- phone_call.\n",
    "rule(p2(Call), prefer(r2(Call), r1(Call)), []):- phone_call , at_work.\n",
    "\n",
    "rule(c1(Call), prefer(p2(Call), p1(Call)), []).\n",
    "% And here too for at_work ?\n",
    "rule(c2(Call), prefer(p1(Call), p2(Call)), []):- phone_call , at_work, familly_member(Call).\n",
    "\n",
    "rule(c3(Call), prefer(c2(Call), c1(Call)), []).\n",
    "\n",
    "rule(c4(Call), prefer(c1(Call), c2(Call)), []):- phone_call , at_work, familly_member(Call), at_meeting.\n",
    "\n",
    "rule(c5(Call), prefer(c4(Call), c3(Call)), []).\n",
    "\n",
    "complement(deny(Call), allow(Call)).\n",
    "complement(allow(Call), deny(Call)).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T01:26:02.967456Z",
     "start_time": "2025-04-02T01:26:02.963043Z"
    }
   },
   "source": [
    "# WIP\n",
    "def generate_intermediate_gorgias():\n",
    "    action1, action2 = random.sample(actions, 2)\n",
    "    \n",
    "    fact1 = random.choice(facts)\n",
    "    remaining_facts = [f for f in facts if f != fact1]\n",
    "    fact2 = random.choice(remaining_facts)\n",
    "    remaining_facts = [f for f in remaining_facts if f != fact2]\n",
    "    fact3 = random.choice(remaining_facts)\n",
    "    \n",
    "    rules = []\n",
    "    \n",
    "    rule1 = f\"rule(r1, {action1}, []) :- {fact1}.\"\n",
    "    rule2 = f\"rule(r2, {action2}, []) :- {fact1}.\"\n",
    "    \n",
    "    pref1 = f\"rule(p1, prefer(r1, r2), []) :- {fact1}, {fact3}.\"\n",
    "    pref2 = f\"rule(p2, prefer(r2, r1), []) :- {fact2}, {fact3}.\"\n",
    "    \n",
    "    conflict1 = \"rule(c1, prefer(p2, p1), []).\"\n",
    "    conflict2 = f\"rule(c2, prefer(p1, p2), []) :- {fact2}, neg({fact3}).\"\n",
    "    conflict3 = \"rule(c3, prefer(c2, c1), []).\"\n",
    "    \n",
    "    complement1 = f\"complement({action2}, {action1}).\"\n",
    "    complement2 = f\"complement({action1}, {action2}).\"\n",
    "    \n",
    "    rules.extend([rule1, rule2, pref1, pref2, conflict1, conflict2, conflict3, complement1, complement2])\n",
    "    \n",
    "    return \"\\n\".join(rules)\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:33:58.099985Z",
     "start_time": "2025-04-01T20:33:58.095987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "num_examples = 3\n",
    "gorgias_intermediate_examples = [generate_intermediate_gorgias() for _ in range(num_examples)]\n",
    "\n",
    "for i, example in enumerate(gorgias_intermediate_examples, 1):\n",
    "    print(f\"### Example {i} ###\\n{example}\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Example 1 ###\n",
      "rule(r1, go_to(restaurant), []) :- important_meeting.\n",
      "rule(r2, give_presentation, []) :- important_meeting.\n",
      "rule(p1, prefer(r1, r2), []) :- important_meeting, good_weather.\n",
      "rule(p2, prefer(r2, r1), []) :- weekend, good_weather.\n",
      "rule(c1, prefer(p2, p1), []).\n",
      "rule(c2, prefer(p1, p2), []) :- weekend, neg(good_weather).\n",
      "rule(c3, prefer(c2, c1), []).\n",
      "complement(give_presentation, go_to(restaurant)).\n",
      "complement(go_to(restaurant), give_presentation).\n",
      "\n",
      "### Example 2 ###\n",
      "rule(r1, go_to(cinema), []) :- team_project_due.\n",
      "rule(r2, finish_report, []) :- team_project_due.\n",
      "rule(p1, prefer(r1, r2), []) :- team_project_due, expensive_event.\n",
      "rule(p2, prefer(r2, r1), []) :- got_bonus, expensive_event.\n",
      "rule(c1, prefer(p2, p1), []).\n",
      "rule(c2, prefer(p1, p2), []) :- got_bonus, neg(expensive_event).\n",
      "rule(c3, prefer(c2, c1), []).\n",
      "complement(finish_report, go_to(cinema)).\n",
      "complement(go_to(cinema), finish_report).\n",
      "\n",
      "### Example 3 ###\n",
      "rule(r1, drive_car, []) :- urgent_deadline.\n",
      "rule(r2, visit_doctor, []) :- urgent_deadline.\n",
      "rule(p1, prefer(r1, r2), []) :- urgent_deadline, rainy_day.\n",
      "rule(p2, prefer(r2, r1), []) :- good_weather, rainy_day.\n",
      "rule(c1, prefer(p2, p1), []).\n",
      "rule(c2, prefer(p1, p2), []) :- good_weather, neg(rainy_day).\n",
      "rule(c3, prefer(c2, c1), []).\n",
      "complement(visit_doctor, drive_car).\n",
      "complement(drive_car, visit_doctor).\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code automates the creation of 100 Gorgias code examples and saves them in a structured CSV file."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:39:08.848331Z",
     "start_time": "2025-04-01T20:39:08.843095Z"
    }
   },
   "source": [
    "import csv\n",
    "\n",
    "num_examples = 100\n",
    "\n",
    "gorgias_examples = [generate_beginner_gorgias() for _ in range(num_examples)]\n",
    "\n",
    "with open(\"gorgias_beginner_examples.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Example Number\", \"Gorgias Code\"])\n",
    "    for i, example in enumerate(gorgias_examples, start=1):\n",
    "        writer.writerow([i, example])\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code reads a CSV file containing Gorgias code examples, translates each example into clear English using the OpenAI Chat API, and then writes the original code along with its translation into a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T21:45:49.633034Z",
     "start_time": "2025-03-19T21:42:38.474982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example 1...\n",
      "Processing example 2...\n",
      "Processing example 3...\n",
      "Processing example 4...\n",
      "Processing example 5...\n",
      "Processing example 6...\n",
      "Processing example 7...\n",
      "Processing example 8...\n",
      "Processing example 9...\n",
      "Processing example 10...\n",
      "Processing example 11...\n",
      "Processing example 12...\n",
      "Processing example 13...\n",
      "Processing example 14...\n",
      "Processing example 15...\n",
      "Processing example 16...\n",
      "Processing example 17...\n",
      "Processing example 18...\n",
      "Processing example 19...\n",
      "Processing example 20...\n",
      "Processing example 21...\n",
      "Processing example 22...\n",
      "Processing example 23...\n",
      "Processing example 24...\n",
      "Processing example 25...\n",
      "Processing example 26...\n",
      "Processing example 27...\n",
      "Processing example 28...\n",
      "Processing example 29...\n",
      "Processing example 30...\n",
      "Processing example 31...\n",
      "Processing example 32...\n",
      "Processing example 33...\n",
      "Processing example 34...\n",
      "Processing example 35...\n",
      "Processing example 36...\n",
      "Processing example 37...\n",
      "Processing example 38...\n",
      "Processing example 39...\n",
      "Processing example 40...\n",
      "Processing example 41...\n",
      "Processing example 42...\n",
      "Processing example 43...\n",
      "Processing example 44...\n",
      "Processing example 45...\n",
      "Processing example 46...\n",
      "Processing example 47...\n",
      "Processing example 48...\n",
      "Processing example 49...\n",
      "Processing example 50...\n",
      "Processing example 51...\n",
      "Processing example 52...\n",
      "Processing example 53...\n",
      "Processing example 54...\n",
      "Processing example 55...\n",
      "Processing example 56...\n",
      "Processing example 57...\n",
      "Processing example 58...\n",
      "Processing example 59...\n",
      "Processing example 60...\n",
      "Processing example 61...\n",
      "Processing example 62...\n",
      "Processing example 63...\n",
      "Processing example 64...\n",
      "Processing example 65...\n",
      "Processing example 66...\n",
      "Processing example 67...\n",
      "Processing example 68...\n",
      "Processing example 69...\n",
      "Processing example 70...\n",
      "Processing example 71...\n",
      "Processing example 72...\n",
      "Processing example 73...\n",
      "Processing example 74...\n",
      "Processing example 75...\n",
      "Processing example 76...\n",
      "Processing example 77...\n",
      "Processing example 78...\n",
      "Processing example 79...\n",
      "Processing example 80...\n",
      "Processing example 81...\n",
      "Processing example 82...\n",
      "Processing example 83...\n",
      "Processing example 84...\n",
      "Processing example 85...\n",
      "Processing example 86...\n",
      "Processing example 87...\n",
      "Processing example 88...\n",
      "Processing example 89...\n",
      "Processing example 90...\n",
      "Processing example 91...\n",
      "Processing example 92...\n",
      "Processing example 93...\n",
      "Processing example 94...\n",
      "Processing example 95...\n",
      "Processing example 96...\n",
      "Processing example 97...\n",
      "Processing example 98...\n",
      "Processing example 99...\n",
      "Processing example 100...\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import openai\n",
    "import time\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def translate_gorgias_to_nl(gorgias_code):\n",
    "\n",
    "    prompt = f\"Please translate the following Gorgias program into clear English, without including any additional built-in rules or extra explanations:\\n\\n{gorgias_code}\\n\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages,\n",
    "            temperature=0.5,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        translation = response.choices[0].message['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API call: {e}\")\n",
    "        translation = \"Error in translation.\"\n",
    "\n",
    "    return translation\n",
    "\n",
    "input_file = \"gorgias_beginner_examples.csv\"\n",
    "\n",
    "output_file = \"gorgias_beginner_nl_pairs.csv\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "     open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as outfile:\n",
    "\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = [\"Example Number\", \"Gorgias Code\", \"NL Translation\"]\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        example_number = row[\"Example Number\"]\n",
    "        gorgias_code = row[\"Gorgias Code\"]\n",
    "\n",
    "        print(f\"Processing example {example_number}...\")\n",
    "        nl_translation = translate_gorgias_to_nl(gorgias_code)\n",
    "\n",
    "        writer.writerow({\n",
    "            \"Example Number\": example_number,\n",
    "            \"Gorgias Code\": gorgias_code,\n",
    "            \"NL Translation\": nl_translation\n",
    "        })\n",
    "\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code does the same thing as the previous one, but with an example in the prompt.  This results in significantly improved, more human-like, and syntactically correct responses from the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T01:43:38.039134Z",
     "start_time": "2025-04-02T01:42:59.636676Z"
    }
   },
   "source": [
    "import csv\n",
    "import openai\n",
    "import time\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def translate_gorgias_to_nl(gorgias_code):\n",
    "\n",
    "    prompt = f\"\"\"The Gorgias program :\n",
    "\n",
    ":- dynamic go_out/0, stay_home/0, nice_weather/0, nice_movie_tv/0, invitation_from_friend/0.\n",
    "rule(r1, go_out, []) :- nice_weather.\n",
    "rule(r2, stay_home, []) :- nice_weather.\n",
    "rule(p1, prefer(r1,r2), []).\n",
    "rule(p2, prefer(r2,r1), []) :- nice_movie_tv.\n",
    "rule(c1, prefer(p2,p1), []).\n",
    "rule(c2, prefer(p1,p2) :- invitation_from_friend, []).\n",
    "rule(c3, prefer(c2,c1), []).\n",
    "complement(go_out, stay_home).\n",
    "complement(stay_home, go_out).\n",
    "\n",
    "translates to English as \"When it is nice weather I can go out or stay home. Generally, I prefer to go out but if there is a nice movie on TV I prefer to stay home. However, if I have an invitation from a friend I prefer to go out. I can't at the same time go out and stay home.\".\n",
    "\n",
    "The Gorgias program :\n",
    "\n",
    ":- dynamic accept_call/0, deny_call/0, from_family_member/0.\n",
    "rule(r1, accept_call, []) :-\n",
    "rule(r2, deny_call, []) :-\n",
    "rule(p1, prefer(r2, r1), []).\n",
    "rule(p2, prefer(r1, r2), []) :- from_family_member.\n",
    "rule(c1, prefer(p2, p1), []).\n",
    "complement(accept_call, deny_call).\n",
    "complement(deny_call, accept_call).\n",
    "\n",
    "translates to English as \"I can either accept or deny the call. Generally, I prefer to deny the call but if it is from a family member I prefer to accept the call. I can't at the same time accept and deny the call.\".\n",
    "\n",
    "Please translate the following Gorgias program into clear English by taking into account only the syntax, by disregarding semantic. Output only the translated text, without any labels, introductions, or explanations. :\\n\\n{gorgias_code}\\n\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages,\n",
    "            temperature=0.5,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        translation = response.choices[0].message['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API call: {e}\")\n",
    "        translation = \"Error in translation.\"\n",
    "\n",
    "    return translation\n",
    "\n",
    "input_file = \"gorgias_beginner_examples_modified.csv\"\n",
    "\n",
    "output_file = \"gorgias_beginner_nl_pairs_modified_prompt29.csv\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "     open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as outfile:\n",
    "\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = [\"Example Number\", \"Gorgias Code\", \"NL Translation\"]\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        example_number = row[\"Example Number\"]\n",
    "        gorgias_code = row[\"Gorgias Code\"]\n",
    "\n",
    "        print(f\"Processing example {example_number}...\")\n",
    "        nl_translation = translate_gorgias_to_nl(gorgias_code)\n",
    "\n",
    "        writer.writerow({\n",
    "            \"Example Number\": example_number,\n",
    "            \"Gorgias Code\": gorgias_code,\n",
    "            \"NL Translation\": nl_translation\n",
    "        })\n",
    "\n",
    "        time.sleep(1)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example 1...\n",
      "Processing example 2...\n",
      "Processing example 3...\n",
      "Processing example 4...\n",
      "Processing example 5...\n",
      "Processing example 6...\n",
      "Processing example 7...\n",
      "Processing example 8...\n",
      "Processing example 9...\n",
      "Processing example 10...\n",
      "Processing example 11...\n",
      "Processing example 12...\n",
      "Processing example 13...\n",
      "Processing example 14...\n",
      "Processing example 15...\n",
      "Processing example 16...\n",
      "Processing example 17...\n",
      "Processing example 18...\n",
      "Processing example 19...\n",
      "Processing example 20...\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T01:29:19.284051Z",
     "start_time": "2025-04-02T01:28:32.056965Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from bert_score import score\n",
    "\n",
    "\n",
    "csv_file = \"gorgias_beginner_nl_pairs_modified_prompt22.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "references = df[\"Manual NL Translation\"].tolist()\n",
    "candidates = df[\"NL Translation\"].tolist()\n",
    "\n",
    "P, R, F1 = score(candidates, references, lang=\"en\", verbose=True)\n",
    "\n",
    "for i, (p, r, f1) in enumerate(zip(P, R, F1)):\n",
    "    print(f\"Exemple {df['Example Number'][i]}:\")\n",
    "    print(f\"  Precision : {p.item():.4f}\")\n",
    "    print(f\"  Recall    : {r.item():.4f}\")\n",
    "    print(f\"  F1        : {f1.item():.4f}\")\n",
    "    print(\"----------\")\n",
    "\n",
    "avg_f1 = F1.mean().item()\n",
    "print(f\"F1 moyen sur tous les exemples : {avg_f1:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mmuzz\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 29.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.28 seconds, 71.43 sentences/sec\n",
      "Exemple 1:\n",
      "  Precision : 0.9473\n",
      "  Recall    : 0.9547\n",
      "  F1        : 0.9510\n",
      "----------\n",
      "Exemple 2:\n",
      "  Precision : 0.9717\n",
      "  Recall    : 0.9687\n",
      "  F1        : 0.9702\n",
      "----------\n",
      "Exemple 3:\n",
      "  Precision : 0.9351\n",
      "  Recall    : 0.9328\n",
      "  F1        : 0.9339\n",
      "----------\n",
      "Exemple 4:\n",
      "  Precision : 0.9830\n",
      "  Recall    : 0.9839\n",
      "  F1        : 0.9834\n",
      "----------\n",
      "Exemple 5:\n",
      "  Precision : 0.9517\n",
      "  Recall    : 0.9486\n",
      "  F1        : 0.9502\n",
      "----------\n",
      "Exemple 6:\n",
      "  Precision : 0.9760\n",
      "  Recall    : 0.9750\n",
      "  F1        : 0.9755\n",
      "----------\n",
      "Exemple 7:\n",
      "  Precision : 0.9591\n",
      "  Recall    : 0.9634\n",
      "  F1        : 0.9613\n",
      "----------\n",
      "Exemple 8:\n",
      "  Precision : 0.9790\n",
      "  Recall    : 0.9803\n",
      "  F1        : 0.9796\n",
      "----------\n",
      "Exemple 9:\n",
      "  Precision : 0.9416\n",
      "  Recall    : 0.9577\n",
      "  F1        : 0.9496\n",
      "----------\n",
      "Exemple 10:\n",
      "  Precision : 0.9867\n",
      "  Recall    : 0.9851\n",
      "  F1        : 0.9859\n",
      "----------\n",
      "Exemple 11:\n",
      "  Precision : 0.9528\n",
      "  Recall    : 0.9293\n",
      "  F1        : 0.9409\n",
      "----------\n",
      "Exemple 12:\n",
      "  Precision : 0.9809\n",
      "  Recall    : 0.9799\n",
      "  F1        : 0.9804\n",
      "----------\n",
      "Exemple 13:\n",
      "  Precision : 0.9820\n",
      "  Recall    : 0.9835\n",
      "  F1        : 0.9828\n",
      "----------\n",
      "Exemple 14:\n",
      "  Precision : 0.9464\n",
      "  Recall    : 0.9233\n",
      "  F1        : 0.9347\n",
      "----------\n",
      "Exemple 15:\n",
      "  Precision : 1.0000\n",
      "  Recall    : 1.0000\n",
      "  F1        : 1.0000\n",
      "----------\n",
      "Exemple 16:\n",
      "  Precision : 0.9667\n",
      "  Recall    : 0.9704\n",
      "  F1        : 0.9685\n",
      "----------\n",
      "Exemple 17:\n",
      "  Precision : 0.9503\n",
      "  Recall    : 0.9410\n",
      "  F1        : 0.9456\n",
      "----------\n",
      "Exemple 18:\n",
      "  Precision : 0.9100\n",
      "  Recall    : 0.9157\n",
      "  F1        : 0.9128\n",
      "----------\n",
      "Exemple 19:\n",
      "  Precision : 0.9646\n",
      "  Recall    : 0.9608\n",
      "  F1        : 0.9627\n",
      "----------\n",
      "Exemple 20:\n",
      "  Precision : 0.9641\n",
      "  Recall    : 0.9623\n",
      "  F1        : 0.9632\n",
      "----------\n",
      "F1 moyen sur tous les exemples : 0.9616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T01:29:44.312861Z",
     "start_time": "2025-04-02T01:29:44.293871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "csv_file = \"gorgias_beginner_nl_pairs_modified_prompt22.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "print(\"Colonnes du fichier CSV :\", df.columns.tolist())\n",
    "\n",
    "references = df[\"Manual NL Translation\"].tolist()       # Texte de référence\n",
    "candidates = df[\"NL Translation\"].tolist()       # Traduction candidate\n",
    "\n",
    "tokenized_references = [nltk.word_tokenize(ref) for ref in references]\n",
    "tokenized_candidates = [nltk.word_tokenize(cand) for cand in candidates]\n",
    "\n",
    "list_of_references = [[ref_tokens] for ref_tokens in tokenized_references]\n",
    "\n",
    "print(\"BLEU score par exemple :\")\n",
    "for i, (refs, cand) in enumerate(zip(list_of_references, tokenized_candidates)):\n",
    "    bleu = sentence_bleu(refs, cand)\n",
    "    print(f\"Exemple {df['Example Number'][i]}: {bleu:.4f}\")\n",
    "\n",
    "corpus_bleu_score = corpus_bleu(list_of_references, tokenized_candidates)\n",
    "print(f\"\\nCorpus BLEU score: {corpus_bleu_score:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes du fichier CSV : ['Example Number', 'Gorgias Code', 'NL Translation', 'Manual NL Translation']\n",
      "BLEU score par exemple :\n",
      "Exemple 1: 0.6128\n",
      "Exemple 2: 0.6488\n",
      "Exemple 3: 0.2893\n",
      "Exemple 4: 0.7869\n",
      "Exemple 5: 0.6722\n",
      "Exemple 6: 0.6922\n",
      "Exemple 7: 0.5812\n",
      "Exemple 8: 0.8339\n",
      "Exemple 9: 0.4254\n",
      "Exemple 10: 0.8733\n",
      "Exemple 11: 0.4874\n",
      "Exemple 12: 0.7440\n",
      "Exemple 13: 0.7668\n",
      "Exemple 14: 0.2813\n",
      "Exemple 15: 1.0000\n",
      "Exemple 16: 0.7596\n",
      "Exemple 17: 0.3723\n",
      "Exemple 18: 0.2896\n",
      "Exemple 19: 0.6729\n",
      "Exemple 20: 0.6446\n",
      "\n",
      "Corpus BLEU score: 0.6736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mmuzz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T01:36:59.519610Z",
     "start_time": "2025-04-02T01:36:46.010810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "model_path = download_model(\"wmt20-comet-da\")\n",
    "model = load_from_checkpoint(model_path)\n",
    "\n",
    "csv_file = \"gorgias_beginner_nl_pairs_modified_prompt22.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "references = df[\"Manual NL Translation\"].tolist()\n",
    "candidates = df[\"NL Translation\"].tolist()\n",
    "\n",
    "data = []\n",
    "for ref, cand in zip(references, candidates):\n",
    "    data.append({\n",
    "        \"src\": \"\",\n",
    "        \"mt\": cand,\n",
    "        \"ref\": ref\n",
    "    })\n",
    "\n",
    "output = model.predict(data, batch_size=8, gpus=0)\n",
    "scores = output[\"scores\"] if isinstance(output, dict) and \"scores\" in output else output\n",
    "\n",
    "for i, score in enumerate(scores):\n",
    "    print(f\"Exemple {df['Example Number'][i]}: COMET score = {float(score):.4f}\")\n",
    "\n",
    "avg_score = sum(float(s) for s in scores) / len(scores)\n",
    "print(f\"\\nCOMET score moyen sur l'ensemble des exemples : {avg_score:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wmt20-comet-da is already in cache.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\mmuzz\\.cache\\torch\\unbabel_comet\\wmt20-comet-da\\checkpoints\\model.ckpt`\n",
      "Encoder model frozen.\n",
      "C:\\Users\\mmuzz\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\core\\saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\mmuzz\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:03<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple 1: COMET score = 0.6903\n",
      "Exemple 2: COMET score = 0.7221\n",
      "Exemple 3: COMET score = 0.5315\n",
      "Exemple 4: COMET score = 0.7361\n",
      "Exemple 5: COMET score = 0.5841\n",
      "Exemple 6: COMET score = 0.6882\n",
      "Exemple 7: COMET score = 0.5743\n",
      "Exemple 8: COMET score = 0.7539\n",
      "Exemple 9: COMET score = 0.6205\n",
      "Exemple 10: COMET score = 0.8056\n",
      "Exemple 11: COMET score = 0.4532\n",
      "Exemple 12: COMET score = 0.8351\n",
      "Exemple 13: COMET score = 0.8690\n",
      "Exemple 14: COMET score = 0.0860\n",
      "Exemple 15: COMET score = 0.9535\n",
      "Exemple 16: COMET score = 0.6899\n",
      "Exemple 17: COMET score = 0.2492\n",
      "Exemple 18: COMET score = 0.2653\n",
      "Exemple 19: COMET score = 0.7713\n",
      "Exemple 20: COMET score = 0.7763\n",
      "\n",
      "COMET score moyen sur l'ensemble des exemples : 0.6328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
